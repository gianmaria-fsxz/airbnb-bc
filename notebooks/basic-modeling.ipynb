{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 30\n",
    "pd.options.display.max_rows = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_rename(path: str) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except(FileNotFoundError):\n",
    "        print(\"File does not exists!\")\n",
    "        return None\n",
    "    \n",
    "    #basic columns renaming\n",
    "    rename_mapper = {k:re.sub(\"[^A-Z|_]\", \"\", k.lower().replace(\" \", \"_\") ,0,re.IGNORECASE) for k in df.columns}\n",
    "    return df.rename(columns=rename_mapper)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_and_rename(\"../bc_data/BrightonPerformanceData.csv\")\n",
    "\n",
    "geo_columns = ['latitude', 'longitude', 'zipcode', 'city']\n",
    "constants = ['scraped_during_month', 'country_code', 'currency_native']\n",
    "not_useful = ['property_type', 'airbnb_host_id', 'last_seen']\n",
    "cols_to_drop = geo_columns + constants + not_useful\n",
    "df = df.drop(cols_to_drop, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.sort_values(by=['airbnb_property_id', 'reporting_month'], inplace=True)\n",
    "\n",
    "# Shift the reporting_month column by one row for each airbnb_property_id\n",
    "df['next_reporting_month'] = df.groupby('airbnb_property_id')['reporting_month'].shift(-1)\n",
    "\n",
    "# Create a new column that is True if the next month's row exists for that airbnb_property_id\n",
    "df['target'] = ~df['next_reporting_month'].isnull()\n",
    "\n",
    "df  = df[df['reporting_month'] != '2023-10-01'].drop([\"next_reporting_month\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['event_timestamp'] = pd.datetime(df.reporting_month)\n",
    "df['event_timestamp'] = pd.to_datetime(df['reporting_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df1 = df[['airbnb_property_id','event_timestamp'] + ['bedrooms', 'bathrooms']]\n",
    "data_df2 = df[['airbnb_property_id','event_timestamp'] + ['blocked_days', 'available_days', 'occupancy_rate', 'reservation_days']]\n",
    "\n",
    "target_df = df[['airbnb_property_id', 'target','event_timestamp']]\n",
    "\n",
    "# Creating timestamps for the data\n",
    "# timestamps = pd.date_range(\n",
    "#     end=pd.Timestamp.now(), \n",
    "#     periods=len(df), \n",
    "#     freq='D').to_frame(name=\"event_timestamp\", index=False)\n",
    "\n",
    "# # Adding the timestamp column to each DataFrame\n",
    "# target_df = pd.concat(objs=[target_df, timestamps], axis=1)\n",
    "# data_df1 = pd.concat(objs=[data_df1, timestamps], axis=1)\n",
    "# data_df2 = pd.concat(objs=[data_df2, timestamps], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DATA_DIR =\"src/feast/airbnb/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df1.to_parquet(path=os.path.join(DATA_DIR, 'data_df1.parquet'))\n",
    "data_df2.to_parquet(path=os.path.join(DATA_DIR, 'data_df2.parquet'))\n",
    "target_df.to_parquet(path=os.path.join(DATA_DIR, 'target_df.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19488/439514980.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_df['bedrooms'] = training_df['bedrooms'].replace(\"Studio\", \"1\").astype(\"int64\")\n"
     ]
    }
   ],
   "source": [
    "training_df = df[keep_cols]\n",
    "training_df['bedrooms'] = training_df['bedrooms'].replace(\"Studio\", \"1\").astype(\"int64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "def create_model():\n",
    "    class PreprocessDF():\n",
    "        def __init__(self):\n",
    "            self.encoders = {\n",
    "                'sex': OrdinalEncoder(),\n",
    "                'job': OrdinalEncoder(),\n",
    "                'car_type': OrdinalEncoder()\n",
    "            }\n",
    "            # ensure the order and needed columns\n",
    "            self.needed_columns = ['bedrooms', 'bathrooms',\n",
    "       'cleaning_fee', 'blocked_days', 'available_days',\n",
    "       'occupancy_rate', 'reservation_days', 'adr_usd', 'adr_native',\n",
    "       'number_of_reservation', 'revenue_usd', 'revenue_native']\n",
    "    \n",
    "        def fit(self, df, y = None):\n",
    "            for column in df.columns:\n",
    "                if column in self.encoders:\n",
    "                    self.encoders[column].fit(df[[column]])\n",
    "            return self\n",
    "\n",
    "        def transform(self, input_df, y = None):\n",
    "            df = input_df.copy() # creating a copy to avoid changes to original dataset\n",
    "            # for column in self.encoders:\n",
    "            #     df[column] = self.encoders[column].transform(df[[column]])\n",
    "            return df[self.needed_columns].astype('float32')\n",
    "        \n",
    "    # it guarantees that model and preprocessing needed are always togheter\n",
    "    model = Pipeline(steps=[\n",
    "            ('preprocess', PreprocessDF()),\n",
    "            ('classifier', RandomForestClassifier())\n",
    "        ])\n",
    "    \n",
    "    search_params = {'classifier__criterion':['gini'], 'classifier__max_depth':[50, 100], 'classifier__n_estimators': [10, 80]}\n",
    "    # best model with f1, other metrics are only monitored\n",
    "    clf = GridSearchCV(model, search_params, scoring=['f1', 'accuracy', 'balanced_accuracy', 'precision', 'recall', 'roc_auc'], refit='f1', cv=3)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "search_params = {'classifier__criterion':['gini'], 'classifier__max_depth':[50, 100], 'classifier__n_estimators': [10, 80]}\n",
    "model = RandomForestClassifier()\n",
    "clf = create_model()\n",
    "import tempfile\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "experiment_name = 'drivers'\n",
    "existing_exp = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not existing_exp:\n",
    "    experiment_id = mlflow.create_experiment(experiment_name)\n",
    "else:\n",
    "    experiment_id = existing_exp.experiment_id\n",
    "\n",
    "timestamp = datetime.now().isoformat().split(\".\")[0].replace(\":\", \".\")\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name=timestamp) as run:\n",
    "    clf.fit(training_df, training_df['target'])\n",
    "    cv_results = clf.cv_results_\n",
    "    best_index = clf.best_index_\n",
    "    for score_name in [score for score in cv_results if \"mean_test\" in score]:\n",
    "        mlflow.log_metric(score_name, cv_results[score_name][best_index])\n",
    "        mlflow.log_metric(score_name.replace(\"mean\",\"std\"), cv_results[score_name.replace(\"mean\",\"std\")][best_index])\n",
    "\n",
    "    tempdir = tempfile.TemporaryDirectory().name\n",
    "    os.mkdir(tempdir)\n",
    "    filename = \"%s-%s-cv_results.csv\" % ('RandomForest', timestamp)\n",
    "    csv = os.path.join(tempdir, filename)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        pd.DataFrame(cv_results).to_csv(csv, index=False)\n",
    "    \n",
    "    mlflow.log_artifact(csv, \"cv_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weroad-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
